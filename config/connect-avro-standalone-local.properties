# Sample configuration for a standalone Kafka Connect worker that uses Avro serialization and
# integrates the the SchemaConfig Registry.
#
# This sample configuration assumes a local installation of Confluent Platform with all services
# running on their default ports, and a local copy of the connector project.

# Bootstrap Kafka servers. If multiple servers are specified, they should be comma-separated.
bootstrap.servers=confluent:9092

# The converters specify the format of data in Kafka and how to translate it into Connect data.
# Every Connect user will need to configure these based on the format they want their data in
# when loaded from or stored into Kafka
key.converter=io.confluent.connect.avro.AvroConverter
key.converter.schema.registry.url=http://confluent:8081
value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://confluent:8081

# Local storage file for offset data
offset.storage.file.filename=/tmp/connect.offsets

# Confluent Control Center Integration -- uncomment these lines to enable Kafka client interceptors
# that will report audit data that can be displayed and analyzed in Confluent Control Center
# producer.interceptor.classes=io.confluent.connect.scylladb.monitoring.clients.interceptor.MonitoringProducerInterceptor
# consumer.interceptor.classes=io.confluent.connect.scylladb.monitoring.clients.interceptor.MonitoringConsumerInterceptor

# Load our plugin from the directory where a local Maven build creates the plugin archive.
# Paths can be absolute or relative to the directory from where you run the Connect worker.
plugin.path=target/components/packages/